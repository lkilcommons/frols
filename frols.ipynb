{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NARMAX Notes and Codes\n",
    "### Liam M. Kilcommons\n",
    "\n",
    "## Simple Orthogonal Least Squares\n",
    "\n",
    "### Model components\n",
    "Assume we have N observations of some arbitrary number of variables $x$\n",
    "$$\\vec{x}_k=[x_{1,k},x_{2,k},...]^{T}$$ \n",
    "$$k \\in [1,N]$$\n",
    "\n",
    "Suppose further we wish to create an M term/parameter nonlinear model with terms $p$, where each $p$ is some function of the observed variables $x$:\n",
    "$$\\vec{p}_{k} = [p_{1,k},p_{2,k},...] $$\n",
    "$$p_{i,k} = f_{i}([x_{1,k},x_{2,k},...]^{T})$$\n",
    "$$i \\in [1,M]$$\n",
    "\n",
    "### Model Equation\n",
    "The (linear-in-the-parameters) model can be represented with the following matrix equation:\n",
    "$$Y_{[N,1]}=P_{[N,M]}\\theta_{[M,1]} + e_{[N,1]}$$\n",
    "Where $e$ is an error term\n",
    "\n",
    "### Orthogonal Auxillary Model\n",
    "We will create an auxillary model represented by the following matrix equation:\n",
    "$$Y_{[N,1]}=W_{[N,M]}g_{[M,1]} + e_{[N,1]}$$\n",
    "Where the columns of W are orthogonal:\n",
    "$$\n",
    "    \\langle W[:,i],W[:,j] \\rangle = \n",
    "        \\begin{cases} \n",
    "            d_{i} & i = j \\\\\n",
    "            0 & i \\ne j\n",
    "        \\end{cases}\n",
    "$$\n",
    "($\\langle .,. \\rangle$ is the inner (dot) product)  \n",
    "\n",
    "Note that this also implies that:\n",
    "$$W^{T}W = \\Lambda = diag([d_1,d_2,...d_M])$$\n",
    "\n",
    "If the regression matrix P from the original model is full rank in columns (its columns span an M dimensional vector space)  \n",
    "Then P can be QR decomposed into a matrix with orthogonal columns $W$ and an upper triangular matrix $A$:\n",
    "$$P_{[N,M]} = W_{[N,M]}A_{[M,M]}$$\n",
    "\n",
    "It can be shown that the following relationship between the original regression equation and the auxillary one holds:\n",
    "$$ Y = Wg + e = (PA^{-1})(A\\theta) + e $$\n",
    "\n",
    "Then, due to orthogonality, performing the auxillary regression to get coefficient vector $g$ becomes a simple matrix multipliction:  \n",
    "\n",
    "$$ g_{[M,1]} = \\Lambda_{[M,M]}^{-1}W^{T}_{[M,N]}Y_{[N,1]} $$\n",
    "\n",
    "Then the original model coefficient vectors $\\theta$ can be calculated by performing Gram-Schmidt on the following:\n",
    "$$ A_{[M,M]}\\theta_{[M,1]} = g_{[M,1]} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foreward Regression Orthongonal Least Squares (FROLS)\n",
    "\n",
    "As in simple OLS above, let the N values of M model terms $p$ (functions of N observations of arbitrary number of model variables $x$) be:\n",
    "   $$p_{i}(k), k \\in [1,N], i \\in [1,M]$$\n",
    "   \n",
    "In matrix notation:\n",
    "   $$P_{[N,M]}[k,i] = p_{i}(k)$$\n",
    "\n",
    "Also, let the overall regression be: the column vector of outputs you want to predict:\n",
    "    $$Y_{[N,1]} = P_{[N,M]}\\theta_{[1,M]} + e_{[1,N]}$$\n",
    "\n",
    "Then, let the columns of P form a set D:\n",
    "    $$D = \\{P[:,1],P[:,2],...,P[:,M]\\}$$\n",
    "    \n",
    "## Step 1: Select First Significant Model Term\n",
    "Define:\n",
    "    $$ D^{1} = D $$\n",
    "    $$ Q = P $$\n",
    "Find regression coefficients and error reducing ratio:\n",
    "    $$ g^{(1)}_{[1,M]}[1,m] = \\frac{ \\langle Y[:,1],Q^{(1)}[:,m] \\rangle }{ \\langle Q^{(1)}[:,m],Q^{(1)}[:,m] \\rangle }$$\n",
    "    $$ ERR^{(1)}_{[1,M]}[1,m] = g[1,m]^{2} \\frac{ \\langle Q^{(1)}[:,m],Q^{(1)}[:,m] \\rangle }{ \\langle Y^{(1)}[:,1],Y^{(1)}[:,1] \\rangle}$$\n",
    "    $$ m \\in [1,M] $$\n",
    "    \n",
    "Find the index $l \\in [1,M]$ which maximizes ERR:\n",
    "$$ l_{1} = argmax(ERR^{(1)}[1,:]) $$\n",
    "\n",
    "## Step 2,3,... : Select More Significant Model Terms\n",
    "Define, for s-th step:\n",
    "    $$ D^{(s-1)}=\\{P[:,l_{1}],P[:,l_{2}],...,P[:,l_{s-1}]\\} $$  \n",
    "    $$ D = D - D^{(s-1)}$$\n",
    "Define $Q$ as a matrix of orthogonal columns spanning the same space as the already selected columns of $P$ (QR factorization):\n",
    "    $$P^{(s-1)} = QR $$\n",
    "    $$P^{(s-1)}=[P[:,l_{1}],P[:,l_{2}],...,P[:,l_{s-1}]]$$\n",
    "Remove the information in $P^{(s-1)}$, via $Q$ from the remaining (unselected) columns of P:\n",
    "    $$ Q^{(s)}_{[N,M_{s}]}[:,m] = P[:,m] - \\sum_{r=1}^{M0} \\frac{ \\langle P[:,m],Q[:,r] \\rangle }{ \\langle Q[:,r], Q[:,r]\\rangle } Q[:,r] $$  \n",
    "\n",
    "Find the s-th-iteration-specific regression coefficients and Error Reducing Ratio:\n",
    "    $$ M_{s} = M-s $$\n",
    "    $$ g^{(s)}_{[1,M_{s}]}[1,m] = \\frac{ \\langle Y[:,1],Q^{(s)}[:,m] \\rangle }{ \\langle Q^{(s)}[:,m],Q^{(s)}[:,m] \\rangle }$$\n",
    "    $$ ERR^{(s)}_{[1,M_{s}]}[1,m] = g^{(s)}[1,m]^{2} \\frac{ \\langle Q^{(s)}[:,m],Q^{(s)}[:,m] \\rangle }{ \\langle Y[:,1],Y[:,1] \\rangle}$$\n",
    "    $$ m \\in [1,M] \\text{ excluding } [l_1,l_2,...,l_s] $$\n",
    "    \n",
    "## Penultimate Step, Put Together Final Auxillary Model\n",
    "\n",
    "Define final orthogonal auxillary term matrix (assume M0 terms were found):\n",
    "    $$ Q_{[N,M0]} = [Q^{1}[:,l_1],Q^{2}[:,l_2],...,Q^{M0}[:,l_{M0}]] $$\n",
    "Define M0 final auxillary regression coefficients:\n",
    "    $$ g_{[1,M0]} = [g^{1}[:,l_1],g^{2}[:,l_2],...,g^{M0}[:,l_{M0}]] $$\n",
    "Define final triangular matrix (i.e. the R of QR):\n",
    "    $$ A_{[M0,M0]}[r,s] =  \\frac{ \\langle Q[:,r],P[:,l_s] \\rangle }{ \\langle Q[:,r], Q[:,r]\\rangle } r \\in [1,s-1]$$\n",
    "    $$ A_{[M0,M0]}[s,s] = 1 $$\n",
    "Define final ERR:\n",
    "    $$ ERR_{[1,M0]}[1,s] = ERR^{(s)}[1,l_s] $$ \n",
    "    \n",
    "## Final Step: Transform Final Auxillary Model To Final Model\n",
    "\n",
    "Define the final model as:\n",
    "$$ Y_{[N,M0]} = P_{l,[N,M0]}\\beta_{l,[M0,1]} + e_{[M0,1]} $$\n",
    "$$ P_{l} = [P[:,l_1],P[:,l_2],...,P[:,l_{M0}]] $$ \n",
    "To find coefficients $\\beta$ solve (scipy.linalg.solve_triangular):\n",
    "$$ A\\beta = g $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as linalg\n",
    "def inner_product_ratio(x,y):\n",
    "    return np.dot(x.T,y)/np.dot(y.T,y)\n",
    "def proj(u,v):\n",
    "    return np.dot(u.T,v)/np.dot(u.T,u)*u\n",
    "def calc_g(Q,Y):\n",
    "    n_params = Q.shape[1]\n",
    "    g = np.full((n_params,1),np.nan)\n",
    "    for i_param in range(n_params):\n",
    "        g[i_param]=inner_product_ratio(Y,Q[:,i_param])\n",
    "    return g\n",
    "def calc_err(g,Q,Y):\n",
    "    sigma = np.dot(Y.T,Y)\n",
    "    n_params = Q.shape[1]\n",
    "    err = np.full_like(g,np.nan)\n",
    "    for i_param in range(n_params):\n",
    "        err[i_param]=g[i_param]**2*np.dot(Q[:,i_param].T,Q[:,i_param])/sigma\n",
    "    return err\n",
    "def calc_esr(err):\n",
    "    return 1-np.sum(err.flatten())\n",
    "def orthogonalize(P):\n",
    "    (Q,R) = np.linalg.qr(P,mode='reduced')\n",
    "    return Q\n",
    "\n",
    "def remove_B_columns_from_A(A,B):\n",
    "    n_params = A.shape[1]\n",
    "    n_cols = B.shape[1]\n",
    "    A_new = A.copy()\n",
    "    for i_param in range(n_params):\n",
    "        for i_col in range(n_cols):\n",
    "            A_new[:,i_param] -= proj(B[:,i_col],A[:,i_param])\n",
    "    return A_new\n",
    "def solve_triangular(A,B):\n",
    "    \"\"\"\n",
    "    Solve Ax = B for x when A is upper triangular, and has unit diagonal\n",
    "    \"\"\"\n",
    "    x = linalg.solve_triangular(A,B,unit_diagonal=True)\n",
    "    return x\n",
    "def frols(Y,P,pcols=None,term_thresh=1.0e-3,max_steps=1000):\n",
    "    l_list = [] # List of indicies (columns of P) which have been selected for model\n",
    "    g_list = [] # List of coefficients to retained from each step (i.e. for step s g^s[:,l_s])\n",
    "    Q_list = [] # List of columns of step-specific orthogonal matrix Q columns to retain (i.e. Q^s[:,l_s])\n",
    "    err_list = [] #List of step-specific error reducing ratios to retain (ERR^s[:,l_s])\n",
    "    if pcols is None:\n",
    "        pcols = {i:'x%d' % (i+1) for i in range(P.shape[1])}\n",
    "    \n",
    "    #Mask\n",
    "    selected = np.zeros((P.shape[1],),dtype=bool)\n",
    "    esr = 1.\n",
    "    i_step = 0\n",
    "    while esr > term_thresh and i_step<P.shape[1]:\n",
    "        selected[l_list] = True\n",
    "        unselected = np.logical_not(selected)\n",
    "        unselected_inds = np.flatnonzero(unselected)\n",
    "        print(\"Not selected %s\" % (str(unselected_inds)))\n",
    "        if i_step==0:\n",
    "            Q = P.copy()\n",
    "            g = calc_g(Q,Y)\n",
    "            err = calc_err(g,Q,Y)\n",
    "            l = np.argmax(err)\n",
    "            l_list.append(l)\n",
    "            g_list.append(g[l])\n",
    "            Q_list.append(Q[:,l])\n",
    "            err_list.append(err[l])\n",
    "        else:\n",
    "            print(\"Shape of P = %s\" % (str(P[:,unselected].shape)))\n",
    "            Q = orthogonalize(P[:,selected])\n",
    "            thisQ = remove_B_columns_from_A(P[:,unselected],Q)\n",
    "            g = calc_g(thisQ,Y)\n",
    "            err = calc_err(g,thisQ,Y)\n",
    "            thisl = np.argmax(err)\n",
    "            l = unselected_inds[thisl]\n",
    "            l_list.append(l)\n",
    "            g_list.append(g[thisl])\n",
    "            Q_list.append(thisQ[:,thisl])\n",
    "            err_list.append(err[thisl])\n",
    "        esr = calc_esr(np.array(err_list))\n",
    "        print(\"Step %d: Selected term at index %d (%s), ERR=%f, ESR=%f\" % (i_step,l_list[-1],pcols[l_list[-1]],err_list[-1],esr))\n",
    "        i_step+=1\n",
    "    #Finally put together the final coefficients\n",
    "    model_str = []\n",
    "    Q_f = np.column_stack(Q_list)\n",
    "    g_f = np.array(g_list).reshape(-1,1)\n",
    "    err_f = np.array(err_list).reshape(-1,1)\n",
    "    n_identified = Q_f.shape[1]\n",
    "    A = np.eye(n_identified)\n",
    "    Pl = P[:,l_list]\n",
    "    #Fill in upper triangluar part of A\n",
    "    for s in range(n_identified):\n",
    "        for r in range(s):\n",
    "            if r==s:\n",
    "                A[r,s]=1.\n",
    "            else:\n",
    "                A[r,s] = np.dot(Q_f[:,r].T,Pl[:,s])/np.dot(Q_f[:,r].T,Q_f[:,r])\n",
    "    coef = solve_triangular(A,g_f)\n",
    "    pred_Y = np.dot(Pl,coef)\n",
    "    modelstr = '+'.join(['%.9f*%s' % (coef[k],pcols[l_list[k]]) for k in range(n_identified)])\n",
    "    print(\"Using model: %s\\n RMSE is %f\" % (modelstr,np.sqrt(np.mean((Y-pred_Y)**2))) )\n",
    "    full_coef = np.zeros((P.shape[1],1))\n",
    "    full_err = np.zeros((P.shape[1],1))\n",
    "    full_coef[l_list]=coef\n",
    "    full_err[l_list]=err_f\n",
    "    return full_coef,full_err,pred_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not selected [0 1 2 3]\n",
      "Step 0: Selected term at index 2 (x3), ERR=0.773707, ESR=0.226293\n",
      "Not selected [0 1 3]\n",
      "Shape of P = (5, 3)\n",
      "Step 1: Selected term at index 0 (x1), ERR=0.172684, ESR=0.053609\n",
      "Not selected [1 3]\n",
      "Shape of P = (5, 2)\n",
      "Step 2: Selected term at index 1 (x2), ERR=0.053523, ESR=0.000086\n",
      "[[ 1.          0.31553398 -0.30582524]\n",
      " [ 0.          1.         -0.40251172]\n",
      " [ 0.          0.          1.        ]]\n",
      "Using model: 0.9967*x3+1.0005*x1+0.9917*x2\n",
      " RMSE is 0.068330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1.00046032],\n",
       "        [ 0.99172293],\n",
       "        [ 0.99669235],\n",
       "        [ 0.        ]]), array([[ 0.17268374],\n",
       "        [ 0.05352266],\n",
       "        [ 0.77370749],\n",
       "        [ 0.        ]]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pg. 63 of Billings, NARMAX Methods\n",
    "#Example data for NARMAX test\n",
    "X = np.array([[9,-5,5,-1.53],\n",
    "              [1,-1,8,-0.39],\n",
    "              [2,-5,6,-3.26],\n",
    "              [8,-2,0,0.36],\n",
    "              [0,0,9,0.13]])\n",
    "Y = np.array([[9.08],\n",
    "              [7.87],\n",
    "              [3.01],\n",
    "              [5.98],\n",
    "              [9.05]])\n",
    "P = X.copy()\n",
    "pcols = {0:'x1',1:'x2',2:'x3',3:'x4',4:'x5'}\n",
    "frols(Y,P,pcols=pcols)    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
